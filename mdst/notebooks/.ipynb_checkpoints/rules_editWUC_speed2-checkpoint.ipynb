{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries and DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and setup a dictionary in studio fashion\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import MySQLdb\n",
    "import time\n",
    "\n",
    "libraries = {'numpy': np, 'pandas': pd, 're': re, \"collections\": collections, \"time\": time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set database credentials and create a database\n",
    "dsn_database = \"mdst_c130\"\n",
    "dsn_hostname = \"localhost\"\n",
    "dsn_port = 3306\n",
    "dsn_uid = \"root\"\n",
    "dsn_pwd = \"root\"\n",
    "\n",
    "conn = MySQLdb.connect(host = dsn_hostname, port = dsn_port, user = dsn_uid, passwd = dsn_pwd, db = dsn_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish the level parameter\n",
    "level = 3\n",
    "\n",
    "if level is None or str(level).lower() not in (\"3\",\"4\",\"5\",\"all\"):\n",
    "    raise Exception, \"requires WUC edit `level` parameter: 3, 4, 5, or all\"\n",
    "level = str(level).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# field names and lookup\n",
    "field_names = ' WUC | MDS | DISC | CN | DIS | DISCREPANCY | CORRECTIVE NARRATIVE | CORRECTIVE ACTION '\n",
    "field_lookup = {\" MDS \":\"Equipment_Designator\",\" WUC \":\"Work_Unit_Code\",\" DISC \": \"Discrepancy_Narrative\", \n",
    "    \" DIS \":\"Discrepancy_Narrative\", \" CN \":\"Corrective_Narrative\", \" DISCREPANCY \": \"Discrepancy_Narrative\",\n",
    "    \" CORRECTIVE NARRATIVE \":\"Corrective_Narrative\", \" CORRECTIVE ACTION \": \"Corrective_Narrative\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25,323 WUCs in the data.\n"
     ]
    }
   ],
   "source": [
    "# import work unit codes data\n",
    "table= 'mdst_c130.work_unit_code_names'\n",
    "cut = ''\n",
    "\n",
    "query = \"SELECT Work_Unit_Code, WUC_Narrative FROM {} {}\".format(table, cut)\n",
    "df_wuc_narratives = pd.read_sql(sql=query, con=conn)\n",
    "\n",
    "wucs = df_wuc_narratives.shape[0]\n",
    "print(\"There are {:0,.0f} WUCs in the data.\".format(wucs, df_wuc_narratives.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 201 rules in the data.\n"
     ]
    }
   ],
   "source": [
    "# import rules data\n",
    "table = 'rules_list'\n",
    "cut = ''\n",
    "\n",
    "if level == 'all':\n",
    "    query = \"SELECT rule FROM {} WHERE type = 'Edit' {}\".format(db, cut)\n",
    "else:\n",
    "    query = \"SELECT rule FROM {} WHERE type = 'Edit' AND level = 'Level {}' {}\".format(table, level, cut)\n",
    "\n",
    "# make a list of rules\n",
    "rules = list(pd.read_sql(sql=query, con=conn).rule)\n",
    "\n",
    "print(\"There are {} rules in the data.\".format(len(rules)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e63356a5f411>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_total\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_filter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary_key_fields_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[1;34m'WUC_Rule'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;31m# initialize rules as nan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'WUC_Rule'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# get main data\n",
    "table1 = 'mdst_c130.import_remis_data'\n",
    "table2 = 'mdst_c130.remove_general_support_wucs'\n",
    "\n",
    "# pull wucs and data from the same component\n",
    "primary_key_fields_list = pd.read_sql(sql=\"SHOW KEYS FROM {}\".format(table1), con=conn).Column_name\n",
    "primary_key_fields = ', '.join(primary_key_fields_list)\n",
    "query = \"\"\"SELECT {}, Work_Unit_Code, Equipment_Designator, Discrepancy_Narrative, Corrective_Narrative FROM {}\"\"\".format(primary_key_fields, table1)\n",
    "df_total = pd.read_sql(sql=query, con=conn)\n",
    "\n",
    "# create a filter dataset\n",
    "df_filter = pd.read_sql(sql=\"SELECT {} FROM {}\".format(primary_key_fields, table2), con=conn)\n",
    "\n",
    "# drop specific column if overlapping\n",
    "if 'Work_Unit_Code' in list(df_filter.columns):\n",
    "    df_total.drop('Work_Unit_Code', axis=1, inplace=True)\n",
    "\n",
    "# inner join to limit solution set\n",
    "df1 = df_total.merge(df_filter, on=list(primary_key_fields_list))\n",
    "\n",
    "if 'WUC_Rule' not in list(df.columns):\n",
    "    # initialize rules as nan\n",
    "    df['WUC_Rule'] = np.nan\n",
    "    # data size\n",
    "    \n",
    "df2 = df1.copy()\n",
    "\n",
    "rows_total = df_total.shape[0]  \n",
    "rows = df1.shape[0]\n",
    "print(\"There are {:0,.0f} rows in the original data and {:0,.0f} rows in the final data.\".format(rows_total, rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_rule_number(existing_val, new_val, libraries):\n",
    "    # add rule number to rule column\n",
    "    pd = libraries[\"pandas\"]\n",
    "    \n",
    "    if existing_val is None or pd.isnull(existing_val):\n",
    "        return str(new_val)\n",
    "    else:\n",
    "        return ', '.join([str(existing_val), str(new_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rule(rule, df, df_wuc_narratives, libraries, field_names, field_lookup, debug=False):\n",
    "    \"\"\"function takes a rule text and the df, prints rule number how many edits were made \n",
    "    and returns a filtered dataframe with an updated WUC\"\"\"\n",
    "\n",
    "    re = libraries[\"re\"]\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    rule = rule.upper()\n",
    "    \n",
    "    # first check if rule is deleted\n",
    "    if rule.split(' ')[2] == \"DELETED\":\n",
    "        print 'rule {} is deleted'.format(str(re.match(r'\\w+.\\s?\\'[0-9\\-]+',rule).group()))\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        rule_name = re.match(r'\\w+.\\s?\\'[0-9\\-]+',rule).group()\n",
    "        rule_number = re.match(r'\\w+',rule).group()\n",
    "    except:\n",
    "        rule_name = re.match(r'\\w+',rule).group()\n",
    "        rule_number = rule_name\n",
    "\n",
    "    # what to change wuc to\n",
    "    change_to_str = re.search(r'\\s+then\\s+change\\s+wuc to\\s+|\\s+then wuc equals\\s+|\\s+then wuc =\\s+|\\s+then change\\s+the wuc to\\s+', \n",
    "        rule, flags=re.IGNORECASE)\n",
    "    new_wuc = re.search(r\"[0-9a-z]+\", rule[change_to_str.end():], flags=re.IGNORECASE).group()\n",
    "    if debug:\n",
    "        print 'originally {} rows'.format(df.shape[0])\n",
    "        print 'change wuc to: ' + repr(new_wuc)\n",
    "\n",
    "    # find text for new_wuc\n",
    "    try:\n",
    "        wuc_narr = df_wuc_narratives[df_wuc_narratives.Work_Unit_Code == new_wuc].WUC_Narrative.iloc[0]\n",
    "    except (IndexError, AttributeError):\n",
    "        wuc_narr = ''\n",
    "    \n",
    "    # remove change wuc to for looping through categories\n",
    "    rule = rule[0:change_to_str.start()]\n",
    "    if debug:\n",
    "        print 'rule after removing new wuc info: ' + rule\n",
    "\n",
    "    # initialy all indices may be changed\n",
    "    matching_indices_defined = set(df.index)\n",
    "    # to account for joining contains/not contains clauses with ORs, need to maintain list of 'pending' indices \n",
    "    #   that must be anded with what's already defined (e.g. by WUC matches)\n",
    "    matching_indices_pending = matching_indices_defined\n",
    "    \n",
    "    # loop through specific category matches (i.e. field names)\n",
    "    # assume always an 'AND' between category matches (e.g. CN = 'X' AND DISC = 'Y')\n",
    "    rule_split = re.split(field_names, rule, flags=re.IGNORECASE)\n",
    "    for ii, cat in enumerate(re.findall(field_names, rule, re.IGNORECASE)):\n",
    "        cat_field = field_lookup[cat]\n",
    "        rule_element = rule_split[1+ii]  # first piece is the wuc begins with, skip\n",
    "        \n",
    "        if debug:\n",
    "            print cat_field\n",
    "        # loop through contains or not contains clauses similar to looping through categories\n",
    "        #   cannot assume an 'AND' between all the contains/not contains matches\n",
    "        #   identify indices to adjust, using set unions and set intersections for ORs and ANDs\n",
    "        match_type_options_regex = r\"\\s*contains\\s*|\\s*does not contain\\s*|\\s*=\\s*|\\s*equals\\s*|\\s*begins with\\s*|\\s*ends with\\s*\"\n",
    "        if debug:\n",
    "            print 'category element: ' + rule_element\n",
    "        match_type_split = re.split(match_type_options_regex, rule_element, flags=re.IGNORECASE)\n",
    "        match_type_split = [item for item in match_type_split if item] # remove empty strings\n",
    "        for jj, match_type in enumerate(re.findall(match_type_options_regex, rule_element, re.IGNORECASE)):\n",
    "            joined_by_and = True\n",
    "            # contains or not contains (true or false)\n",
    "            #cat_direction = re.search(\"\\s?contains|\\s?does not contain\\s?\\\"?\", rule_element, flags=re.IGNORECASE).group()\n",
    "            #match_type = re.search('contains', cat_direction, re.IGNORECASE) is not None\n",
    "            if debug:\n",
    "                print \"match type: \" + match_type # ('contains' if match_type else 'does not contain')\n",
    "\n",
    "            # find what we're looking for for this category\n",
    "            #cat_remaining = rule_element[len(cat_direction):]  # rest of phrase after contains/not contains. \n",
    "            #   also remove any trailing 'and the', e.g. for wuc ends with in \"WUC begins with 14 AND ENDS WITH 00 OR 99 AND the CN contains \"BOOST\"\"\n",
    "            cat_remaining = re.sub(' THE$', '', match_type_split[jj].strip())\n",
    "            cat_remaining = re.sub(' AND$', '', cat_remaining).strip()\n",
    "            # remove trailing OR from contains/not contains cluse and take note\n",
    "            if re.search(' OR$', cat_remaining, flags=re.IGNORECASE):\n",
    "                cat_remaining = re.split(' OR$', cat_remaining, flags=re.IGNORECASE)[0].strip()\n",
    "                joined_by_and = False\n",
    "\n",
    "            if debug:\n",
    "                print 'cat_remaining: ' + cat_remaining\n",
    "            #cat_ANDs = cat_remaining.count(' AND ', re.IGNORECASE) # this is wrong if multiple cateogries linked by ANDs\n",
    "            cat_ORs = cat_remaining.count(' OR ')\n",
    "\n",
    "            # group all the ands into a list\n",
    "            cat_remaining_andsplit = re.split(' AND ', cat_remaining, flags=re.IGNORECASE)\n",
    "\n",
    "            # multiple category elements are linked by 'ands'.  remove these to avoid tricking the 'and' search later\n",
    "            # remove any empty strings, '', at the end of these categories\n",
    "            cat_remaining_andsplit = [element for element in cat_remaining_andsplit if element]\n",
    "\n",
    "            # if any ORs, split them off from the last item in the AND list\n",
    "            if cat_ORs > 0:\n",
    "                cat_remaining_orsplit = cat_remaining_andsplit.pop()\n",
    "                cat_remaining_orsplit = re.split(' OR ', cat_remaining_orsplit, flags=re.IGNORECASE)\n",
    "                # strip and remove quotes and parentheses from all the OR matches.\n",
    "                cat_pattern_ORs = [blurb.strip().strip(',').replace(\"\\\"\",\"\").replace(\"(\",\"\").replace(\")\",\"\") for blurb in cat_remaining_orsplit]\n",
    "                cat_pattern_ORs = '|'.join(cat_pattern_ORs)\n",
    "                # periods are wildcards, so escape\n",
    "                cat_pattern_ORs = cat_pattern_ORs.replace('.',r'\\.')\n",
    "                if debug:\n",
    "                    print 'filter for ' + cat_pattern_ORs\n",
    "\n",
    "                if 'CONTAINS' in match_type:\n",
    "                    # update indices: ensure new matches always match to the defined (e.g. WUC-starts-with) indice matches\n",
    "                    #  if joined by and: pending = intersection(pending, intersection(defined, new-matches))\n",
    "                    #  if joined by or: pending = union(pending, intersection(defined, new-matches))\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.contains(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.contains(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE))].index)))\n",
    "                elif 'NOT CONTAIN' in match_type:  # exclude if any words match\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[~(df[cat_field].str.contains(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[~(df[cat_field].str.contains(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE))].index)))\n",
    "                elif '=' in match_type or 'EQUALS' in match_type:\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.match(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE)) & (df[cat_field].str.len() == len(cat_pattern_ORs.split('|')[0]))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.match(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE)) & (df[cat_field].str.len() == len(cat_pattern_ORs.split('|')[0]))].index)))                            \n",
    "                elif 'BEGINS WITH' in match_type:\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.startswith(tuple(cat_pattern_ORs.split('|')),na=False))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.startswith(tuple(cat_pattern_ORs.split('|')),na=False))].index)))\n",
    "                else:  # ends with\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.endswith(tuple(cat_pattern_ORs.split('|')),na=False))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.endswith(tuple(cat_pattern_ORs.split('|')),na=False))].index)))\n",
    "\n",
    "            if debug:\n",
    "                    print 'after matching any ORs: {} matches'.format(len(matching_indices_pending))\n",
    "\n",
    "            # strip and remove quotes from all the AND matches. escape periods (wildcards). keep a list\n",
    "            cat_pattern_ANDs = [blurb.strip().strip(',').replace(\"\\\"\",\"\").replace('.',r'\\.') for blurb in cat_remaining_andsplit]\n",
    "\n",
    "            # filter and phrases for this category\n",
    "            for cat_AND in cat_pattern_ANDs:\n",
    "                if debug:\n",
    "                    print 'filter for ' + cat_AND\n",
    "\n",
    "                if 'CONTAINS' in match_type:    \n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.contains(pat=cat_AND,na=False,flags=re.IGNORECASE))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.contains(pat=cat_AND,na=False,flags=re.IGNORECASE))].index)))\n",
    "                elif 'NOT CONTAIN' in match_type:\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[~(df[cat_field].str.contains(pat=cat_AND,na=False,flags=re.IGNORECASE))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[~(df[cat_field].str.contains(pat=cat_AND,na=False,flags=re.IGNORECASE))].index)))\n",
    "                elif '=' in match_type or 'EQUALS' in match_type:\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.match(pat=cat_AND,na=False,flags=re.IGNORECASE))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.match(pat=cat_AND,na=False,flags=re.IGNORECASE))].index)))\n",
    "                elif 'BEGINS WITH' in match_type:\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.startswith(cat_AND,na=False))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.startswith(cat_AND,na=False))].index)))\n",
    "                else:  # ends with\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.endswith(cat_AND,na=False))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.endswith(cat_AND,na=False))].index)))\n",
    "\n",
    "            if debug:\n",
    "                print 'after filtering match type: {} matches'.format(len(matching_indices_pending))\n",
    "\n",
    "        # finished looping through contains/not contains clauses for this field/category. update defined indices\n",
    "        matching_indices_defined = matching_indices_pending\n",
    "\n",
    "        if debug:\n",
    "            print 'after filtering category: {} matches'.format(len(matching_indices_defined))\n",
    "        if len(matching_indices_defined) == 0:\n",
    "            # print 'rule {} changed 0 records to wuc {}: {}'.format(str(rule_name), new_wuc, wuc_narr)\n",
    "            return None\n",
    "\n",
    "    # filter\n",
    "    df = df.filter(items=matching_indices_defined, axis=\"index\")\n",
    "    if debug:\n",
    "        print 'matching_indices: ' + str(matching_indices_defined)\n",
    "        print 'filtered df: '\n",
    "        print df\n",
    "    # update wuc and rule\n",
    "    df.loc[:,'Work_Unit_Code'] = new_wuc\n",
    "    if 'WUC_Rule' in df.columns:\n",
    "        df.loc[:, 'WUC_Rule'] = df.WUC_Rule.apply(func=append_rule_number, args=(rule_number,libraries))\n",
    "    else:\n",
    "        df.loc[:,'WUC_Rule'] = str(rule_number)\n",
    "    \n",
    "    # print 'rule {} changed {:0,.0f} records to wuc {}: {}'.format(str(rule_name), df.shape[0], new_wuc, wuc_narr)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rule_fast(rule, df, df_wuc_narratives, libraries, field_names, field_lookup, debug=False):\n",
    "    \"\"\"function takes a rule text and the df, prints rule number how many edits were made \n",
    "    and returns a filtered dataframe with an updated WUC\"\"\"\n",
    "\n",
    "    re = libraries[\"re\"]\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    rule = rule.upper()\n",
    "    \n",
    "    # first check if rule is deleted\n",
    "    if rule.split(' ')[2] == \"DELETED\":\n",
    "        print 'rule {} is deleted'.format(str(re.match(r'\\w+.\\s?\\'[0-9\\-]+',rule).group()))\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        rule_name = re.match(r'\\w+.\\s?\\'[0-9\\-]+',rule).group()\n",
    "        rule_number = re.match(r'\\w+',rule).group()\n",
    "    except:\n",
    "        rule_name = re.match(r'\\w+',rule).group()\n",
    "        rule_number = rule_name\n",
    "\n",
    "    # what to change wuc to\n",
    "    change_to_str = re.search(r'\\s+then\\s+change\\s+wuc to\\s+|\\s+then wuc equals\\s+|\\s+then wuc =\\s+|\\s+then change\\s+the wuc to\\s+', \n",
    "        rule, flags=re.IGNORECASE)\n",
    "    new_wuc = re.search(r\"[0-9a-z]+\", rule[change_to_str.end():], flags=re.IGNORECASE).group()\n",
    "    if debug:\n",
    "        print 'originally {} rows'.format(df.shape[0])\n",
    "        print 'change wuc to: ' + repr(new_wuc)\n",
    "\n",
    "    # find text for new_wuc\n",
    "    try:\n",
    "        wuc_narr = df_wuc_narratives[df_wuc_narratives.Work_Unit_Code == new_wuc].WUC_Narrative.iloc[0]\n",
    "    except (IndexError, AttributeError):\n",
    "        wuc_narr = ''\n",
    "    \n",
    "    # remove change wuc to for looping through categories\n",
    "    rule = rule[0:change_to_str.start()]\n",
    "    if debug:\n",
    "        print 'rule after removing new wuc info: ' + rule\n",
    "\n",
    "    # initialy all indices may be changed\n",
    "    matching_indices_defined = set(df.index)\n",
    "    # to account for joining contains/not contains clauses with ORs, need to maintain list of 'pending' indices \n",
    "    #   that must be anded with what's already defined (e.g. by WUC matches)\n",
    "    matching_indices_pending = matching_indices_defined\n",
    "    \n",
    "    # loop through specific category matches (i.e. field names)\n",
    "    # assume always an 'AND' between category matches (e.g. CN = 'X' AND DISC = 'Y')\n",
    "    rule_split = re.split(field_names, rule, flags=re.IGNORECASE)\n",
    "    for ii, cat in enumerate(re.findall(field_names, rule, re.IGNORECASE)):\n",
    "        cat_field = field_lookup[cat]\n",
    "        rule_element = rule_split[1+ii]  # first piece is the wuc begins with, skip\n",
    "        \n",
    "        if debug:\n",
    "            print cat_field\n",
    "        # loop through contains or not contains clauses similar to looping through categories\n",
    "        #   cannot assume an 'AND' between all the contains/not contains matches\n",
    "        #   identify indices to adjust, using set unions and set intersections for ORs and ANDs\n",
    "        match_type_options_regex = r\"\\s*contains\\s*|\\s*does not contain\\s*|\\s*=\\s*|\\s*equals\\s*|\\s*begins with\\s*|\\s*ends with\\s*\"\n",
    "        if debug:\n",
    "            print 'category element: ' + rule_element\n",
    "        match_type_split = re.split(match_type_options_regex, rule_element, flags=re.IGNORECASE)\n",
    "        match_type_split = [item for item in match_type_split if item] # remove empty strings\n",
    "        for jj, match_type in enumerate(re.findall(match_type_options_regex, rule_element, re.IGNORECASE)):\n",
    "            joined_by_and = True\n",
    "            # contains or not contains (true or false)\n",
    "            #cat_direction = re.search(\"\\s?contains|\\s?does not contain\\s?\\\"?\", rule_element, flags=re.IGNORECASE).group()\n",
    "            #match_type = re.search('contains', cat_direction, re.IGNORECASE) is not None\n",
    "            if debug:\n",
    "                print \"match type: \" + match_type # ('contains' if match_type else 'does not contain')\n",
    "\n",
    "            # find what we're looking for for this category\n",
    "            #cat_remaining = rule_element[len(cat_direction):]  # rest of phrase after contains/not contains. \n",
    "            #   also remove any trailing 'and the', e.g. for wuc ends with in \"WUC begins with 14 AND ENDS WITH 00 OR 99 AND the CN contains \"BOOST\"\"\n",
    "            cat_remaining = re.sub(' THE$', '', match_type_split[jj].strip())\n",
    "            cat_remaining = re.sub(' AND$', '', cat_remaining).strip()\n",
    "            # remove trailing OR from contains/not contains cluse and take note\n",
    "            if re.search(' OR$', cat_remaining, flags=re.IGNORECASE):\n",
    "                cat_remaining = re.split(' OR$', cat_remaining, flags=re.IGNORECASE)[0].strip()\n",
    "                joined_by_and = False\n",
    "\n",
    "            if debug:\n",
    "                print 'cat_remaining: ' + cat_remaining\n",
    "            #cat_ANDs = cat_remaining.count(' AND ', re.IGNORECASE) # this is wrong if multiple cateogries linked by ANDs\n",
    "            cat_ORs = cat_remaining.count(' OR ')\n",
    "\n",
    "            # group all the ands into a list\n",
    "            cat_remaining_andsplit = re.split(' AND ', cat_remaining, flags=re.IGNORECASE)\n",
    "\n",
    "            # multiple category elements are linked by 'ands'.  remove these to avoid tricking the 'and' search later\n",
    "            # remove any empty strings, '', at the end of these categories\n",
    "            cat_remaining_andsplit = [element for element in cat_remaining_andsplit if element]\n",
    "\n",
    "            # if any ORs, split them off from the last item in the AND list\n",
    "            if cat_ORs > 0:\n",
    "                cat_remaining_orsplit = cat_remaining_andsplit.pop()\n",
    "                cat_remaining_orsplit = re.split(' OR ', cat_remaining_orsplit, flags=re.IGNORECASE)\n",
    "                # strip and remove quotes and parentheses from all the OR matches.\n",
    "                cat_pattern_ORs = [blurb.strip().strip(',').replace(\"\\\"\",\"\").replace(\"(\",\"\").replace(\")\",\"\") for blurb in cat_remaining_orsplit]\n",
    "                cat_pattern_ORs = '|'.join(cat_pattern_ORs)\n",
    "                # periods are wildcards, so escape\n",
    "                cat_pattern_ORs = cat_pattern_ORs.replace('.',r'\\.')\n",
    "                if debug:\n",
    "                    print 'filter for ' + cat_pattern_ORs\n",
    "\n",
    "                if 'CONTAINS' in match_type:\n",
    "                    # update indices: ensure new matches always match to the defined (e.g. WUC-starts-with) indice matches\n",
    "                    #  if joined by and: pending = intersection(pending, intersection(defined, new-matches))\n",
    "                    #  if joined by or: pending = union(pending, intersection(defined, new-matches))\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.contains(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.contains(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE))].index)))\n",
    "                elif 'NOT CONTAIN' in match_type:  # exclude if any words match\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[~(df[cat_field].str.contains(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[~(df[cat_field].str.contains(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE))].index)))\n",
    "                elif '=' in match_type or 'EQUALS' in match_type:\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.match(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE)) & (df[cat_field].str.len() == len(cat_pattern_ORs.split('|')[0]))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.match(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE)) & (df[cat_field].str.len() == len(cat_pattern_ORs.split('|')[0]))].index)))                            \n",
    "                elif 'BEGINS WITH' in match_type:\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.startswith(tuple(cat_pattern_ORs.split('|')),na=False))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.startswith(tuple(cat_pattern_ORs.split('|')),na=False))].index)))\n",
    "                else:  # ends with\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.endswith(tuple(cat_pattern_ORs.split('|')),na=False))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.endswith(tuple(cat_pattern_ORs.split('|')),na=False))].index)))\n",
    "\n",
    "            if debug:\n",
    "                    print 'after matching any ORs: {} matches'.format(len(matching_indices_pending))\n",
    "\n",
    "            # strip and remove quotes from all the AND matches. escape periods (wildcards). keep a list\n",
    "            cat_pattern_ANDs = [blurb.strip().strip(',').replace(\"\\\"\",\"\").replace('.',r'\\.') for blurb in cat_remaining_andsplit]\n",
    "\n",
    "            # filter and phrases for this category\n",
    "            for cat_AND in cat_pattern_ANDs:\n",
    "                if debug:\n",
    "                    print 'filter for ' + cat_AND\n",
    "\n",
    "                if 'CONTAINS' in match_type:    \n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.contains(pat=cat_AND,na=False,flags=re.IGNORECASE))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.contains(pat=cat_AND,na=False,flags=re.IGNORECASE))].index)))\n",
    "                elif 'NOT CONTAIN' in match_type:\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[~(df[cat_field].str.contains(pat=cat_AND,na=False,flags=re.IGNORECASE))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[~(df[cat_field].str.contains(pat=cat_AND,na=False,flags=re.IGNORECASE))].index)))\n",
    "                elif '=' in match_type or 'EQUALS' in match_type:\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.match(pat=cat_AND,na=False,flags=re.IGNORECASE))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.match(pat=cat_AND,na=False,flags=re.IGNORECASE))].index)))\n",
    "                elif 'BEGINS WITH' in match_type:\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.startswith(cat_AND,na=False))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.startswith(cat_AND,na=False))].index)))\n",
    "                else:  # ends with\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.endswith(cat_AND,na=False))].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.endswith(cat_AND,na=False))].index)))\n",
    "\n",
    "            if debug:\n",
    "                print 'after filtering match type: {} matches'.format(len(matching_indices_pending))\n",
    "\n",
    "        # finished looping through contains/not contains clauses for this field/category. update defined indices\n",
    "        matching_indices_defined = matching_indices_pending\n",
    "\n",
    "        if debug:\n",
    "            print 'after filtering category: {} matches'.format(len(matching_indices_defined))\n",
    "        if len(matching_indices_defined) == 0:\n",
    "            # print 'rule {} changed 0 records to wuc {}: {}'.format(str(rule_name), new_wuc, wuc_narr)\n",
    "            return None\n",
    "\n",
    "    # filter\n",
    "    df = df.filter(items=matching_indices_defined, axis=\"index\")\n",
    "    if debug:\n",
    "        print 'matching_indices: ' + str(matching_indices_defined)\n",
    "        print 'filtered df: '\n",
    "        print df\n",
    "    # update wuc and rule\n",
    "    df.loc[:,'Work_Unit_Code'] = new_wuc\n",
    "    if 'WUC_Rule' in df.columns:\n",
    "        df.loc[:, 'WUC_Rule'] = df.WUC_Rule.apply(func=append_rule_number, args=(rule_number,libraries))\n",
    "    else:\n",
    "        df.loc[:,'WUC_Rule'] = str(rule_number)\n",
    "    \n",
    "    # print 'rule {} changed {:0,.0f} records to wuc {}: {}'.format(str(rule_name), df.shape[0], new_wuc, wuc_narr)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_speed(rules, df, df_wuc_narratives, libraries, field_names, field_lookup, version, deb='False'):\n",
    "    total_rows = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    rule_size = len(rules)\n",
    "    df_size = df.shape[0]\n",
    "    \n",
    "    if version == 'original':    \n",
    "        for rule in rules:\n",
    "            df_subset = apply_rule(rule, df, df_wuc_narratives, libraries, field_names, field_lookup)\n",
    "            if df_subset is not None:\n",
    "                total_rows += df_subset.shape[0]\n",
    "                # update WUCs within original dataframe\n",
    "                df.update(df_subset)\n",
    "                # update rules metrics\n",
    "                wuc_group_metrics[str(wuc_group_re.match(rule).group(1))] += df_subset.shape[0]\n",
    "                wuc_group_indices[str(wuc_group_re.match(rule).group(1))] = wuc_group_indices[str(wuc_group_re.match(rule).group(1))].union(set(df_subset.index))\n",
    "                each_rule_indices[str(each_rule_re.match(rule).group(1))] = each_rule_indices[str(each_rule_re.match(rule).group(1))].union(set(df_subset.index))\n",
    "\n",
    "    else:    \n",
    "        for rule in rules:\n",
    "            df_subset = apply_rule_fast(rule, df, df_wuc_narratives, libraries, field_names, field_lookup)\n",
    "            if df_subset is not None:\n",
    "                total_rows += df_subset.shape[0]\n",
    "                # update WUCs within original dataframe\n",
    "                df.update(df_subset)\n",
    "                # update rules metrics\n",
    "                wuc_group_metrics[str(wuc_group_re.match(rule).group(1))] += df_subset.shape[0]\n",
    "                wuc_group_indices[str(wuc_group_re.match(rule).group(1))] = wuc_group_indices[str(wuc_group_re.match(rule).group(1))].union(set(df_subset.index))\n",
    "                each_rule_indices[str(each_rule_re.match(rule).group(1))] = each_rule_indices[str(each_rule_re.match(rule).group(1))].union(set(df_subset.index))\n",
    "\n",
    "    end = time.time()\n",
    "    total_time = round(end-start,2)\n",
    "    \n",
    "    \n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"For {} rules and {} maintenance records, it took {} seconds to make {} changes made across all edits\".format(rule_size, df_size, total_time, total_rows))\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    \n",
    "    return total_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt1 = evaluate_speed(rules, df1, df_wuc_narratives, libraries, field_names, field_lookup, deb='False', version = 'original')\n",
    "# tt2 = evaluate_speed(rules, df2, df_wuc_narratives, libraries, field_names, field_lookup, deb='False', version = 'fast')\n",
    "# difference = tt1 - tt2\n",
    "# percent_difference = round(difference / tt1 * 100, 2)\n",
    "# print(\"The improvements resulted in a {} improvement, or {} % upgrade\".format(difference, percent_difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.48\n"
     ]
    }
   ],
   "source": [
    "percent_difference = tt1 - tt2\n",
    "percent_difference = round(difference / tt1 * 100, 2)\n",
    "print(percent_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup metrics and iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wuc_group\n",
    "wuc_group_re = re.compile(\"\\w+.\\s?\\'(\\w+)\")\n",
    "wuc_group_metrics = collections.defaultdict(int)\n",
    "wuc_group_indices = collections.defaultdict(set)\n",
    "\n",
    "# each_rule\n",
    "each_rule_re = re.compile(\"\\w+.\\s?\\'([0-9\\-]+)\")\n",
    "each_rule_indices = collections.defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = 0\n",
    "start = time.time()\n",
    "for rule in rules:\n",
    "    df_subset = apply_rule(rule, df, df_wuc_narratives, libraries, field_names, field_lookup)\n",
    "    if df_subset is not None:\n",
    "        total_rows += df_subset.shape[0]\n",
    "        # update WUCs within original dataframe\n",
    "        df.update(df_subset)\n",
    "        # update rules metrics\n",
    "        wuc_group_metrics[str(wuc_group_re.match(rule).group(1))] += df_subset.shape[0]\n",
    "        wuc_group_indices[str(wuc_group_re.match(rule).group(1))] = wuc_group_indices[str(wuc_group_re.match(rule).group(1))].union(set(df_subset.index))\n",
    "        each_rule_indices[str(each_rule_re.match(rule).group(1))] = each_rule_indices[str(each_rule_re.match(rule).group(1))].union(set(df_subset.index))\n",
    "\n",
    "end = time.time()\n",
    "total_time = end-start\n",
    "print \"It took {} seconds to make {} changes made across all edits (sometimes one row is changed more than once)\".format(total_time, total_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
