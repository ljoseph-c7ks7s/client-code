{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "FIELD_NAMES = ' WUC | MDS | DISC | CN | DIS | DISCREPANCY | CORRECTIVE NARRATIVE '\n",
    "FIELD_LOOKUP = {\" MDS \":\"Equipment_Designator\",\" WUC \":\"Work_Unit_Code\",\" DISC \": \"Discrepancy_Narrative\", \n",
    "\" DIS \":\"Discrepancy_Narrative\", \" CN \":\"Corrective_Narrative\", \" DISCREPANCY \": \"Discrepancy_Narrative\",\n",
    "\" CORRECTIVE NARRATIVE \":\"Corrective_Narrative\"}\n",
    "\n",
    "libraries = {\"pandas\":pd, \"re\":re, \"numpy\":np}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_rule_number(existing_val, new_val, libraries):\n",
    "    # add rule number to rule column\n",
    "    pd = libraries[\"pandas\"]\n",
    "    \n",
    "    if existing_val is None or pd.isnull(existing_val):\n",
    "        return str(new_val)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def apply_rule(rule, df, df_wuc_narratives, libraries, field_names, field_lookup, debug=False):\n",
    "    \"\"\"function takes a rule text and the df, prints rule number how many edits were made \n",
    "    and returns a filtered dataframe with an updated WUC\"\"\"\n",
    "\n",
    "    re = libraries[\"re\"]\n",
    "    np = libraries[\"numpy\"]\n",
    "    \n",
    "    # if rule contains \"D/C\" field, split into two rules - one for DISC, one for CN\n",
    "    if re.search(r'D/C',rule) is not None:\n",
    "        # discrepency\n",
    "        dis_rule = re.sub(r'D/C',r'DISC',rule)\n",
    "        if debug:\n",
    "            print 'D/C rule as DISC rule: {} '.format(dis_rule)\n",
    "        df1 = apply_rule(dis_rule, df, df_wuc_narratives, libraries, field_names, field_lookup, debug=debug)\n",
    "        if df1 is None:\n",
    "            df1 = pd.DataFrame()\n",
    "        \n",
    "        # corrective\n",
    "        cn_rule = re.sub(r'D/C',r'CN',rule)\n",
    "        if debug:\n",
    "            print 'D/C rule as CN rule: {}'.format(cn_rule)\n",
    "        df2 = apply_rule(cn_rule, df, df_wuc_narratives, libraries, field_names, field_lookup, debug=debug)\n",
    "        if df2 is None:\n",
    "            df2 = pd.DataFrame()\n",
    "        \n",
    "        if (len(df1) > 0) and (len(df2) > 0):\n",
    "            df_final = df1.append(df2,ignore_index=False)\n",
    "            df_final['index1'] = df_final.index\n",
    "            df_final.drop_duplicates(inplace=True)\n",
    "            df_final.drop('index1', axis=1)\n",
    "            return df_final\n",
    "        elif len(df1) > 0:\n",
    "            return df1\n",
    "        else:\n",
    "            return df2\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    rule = rule.upper()\n",
    "\n",
    "    # first check if rule is deleted\n",
    "    if rule.split(' ')[2] == \"DELETED\":\n",
    "        print 'rule {} is deleted'.format(str(re.match(r'\\w+.\\s?\\'[0-9\\-]+',rule).group()))\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        rule_name = re.match(r\"\\w+.\\s?[S|A|']F?[0-9\\-\\A-Z]+\",rule).group()\n",
    "        rule_number = re.match(r'\\w+',rule).group()\n",
    "    except:\n",
    "        rule_name = re.match(r'\\w+',rule).group()\n",
    "        rule_number = rule_name\n",
    "\n",
    "    # what to change wuc to\n",
    "    change_to_str = re.search(r'\\s+then\\s+change\\s+wuc to\\s+|\\s+then wuc equals\\s+|\\s+then wuc =\\s+|\\s+then change\\s+the wuc to\\s+', \n",
    "        rule, flags=re.IGNORECASE)\n",
    "    new_wuc = re.search(r\"[0-9a-z]+\", rule[change_to_str.end():], flags=re.IGNORECASE).group()\n",
    "    if debug:\n",
    "        print 'originally {} rows'.format(df.shape[0])\n",
    "        print 'change wuc to: ' + repr(new_wuc)\n",
    "\n",
    "    # find text for new_wuc\n",
    "    try:\n",
    "        wuc_narr = df_wuc_narratives[df_wuc_narratives.Work_Unit_Code == new_wuc].WUC_Narrative.iloc[0]\n",
    "    except (IndexError, AttributeError):\n",
    "        wuc_narr = ''\n",
    "    \n",
    "    # remove change wuc to for looping through categories\n",
    "    rule = rule[0:change_to_str.start()]\n",
    "    if debug:\n",
    "        print 'rule after removing new wuc info: ' + rule\n",
    "\n",
    "    # initially all indices may be changed\n",
    "    matching_indices_defined = set(df.index)\n",
    "    # to account for joining contains/not contains clauses with ORs, need to maintain list of 'pending' indices \n",
    "    #   that must be anded with what's already defined (e.g. by WUC matches)\n",
    "    matching_indices_pending = matching_indices_defined\n",
    "\n",
    "    # create null WUC_RULE column if it does not exist\n",
    "    if 'WUC_Rule' not in df.columns:\n",
    "        df.loc[:,'WUC_Rule'] = np.nan\n",
    "    \n",
    "    # loop through specific category matches (i.e. field names)\n",
    "    # assume always an 'AND' between category matches (e.g. CN = 'X' AND DISC = 'Y')\n",
    "    rule_split = re.split(field_names, rule, flags=re.IGNORECASE)\n",
    "    for ii, cat in enumerate(re.findall(field_names, rule, re.IGNORECASE)):\n",
    "        cat_field = field_lookup[cat]\n",
    "        rule_element = rule_split[1+ii]  # first piece is the wuc begins with, skip\n",
    "        \n",
    "        if debug:\n",
    "            print cat_field\n",
    "        # loop through contains or not contains clauses similar to looping through categories\n",
    "        #   cannot assume an 'AND' between all the contains/not contains matches\n",
    "        #   identify indices to adjust, using set unions and set intersections for ORs and ANDs\n",
    "        match_type_options_regex = r\"\\s*contains\\s*|\\s*does not contain\\s*|\\s*=\\s*|\\s*equals\\s*|\\s*begins with\\s*|\\s*ends with\\s*\"\n",
    "        if debug:\n",
    "            print 'category element: ' + rule_element\n",
    "        match_type_split = re.split(match_type_options_regex, rule_element, flags=re.IGNORECASE)\n",
    "        match_type_split = [item for item in match_type_split if item] # remove empty strings\n",
    "        for jj, match_type in enumerate(re.findall(match_type_options_regex, rule_element, re.IGNORECASE)):\n",
    "            joined_by_and = True\n",
    "            # contains or not contains (true or false)\n",
    "            #cat_direction = re.search(\"\\s?contains|\\s?does not contain\\s?\\\"?\", rule_element, flags=re.IGNORECASE).group()\n",
    "            #match_type = re.search('contains', cat_direction, re.IGNORECASE) is not None\n",
    "            if debug:\n",
    "                print \"match type: \" + match_type # ('contains' if match_type else 'does not contain')\n",
    "\n",
    "            # find what we're looking for for this category\n",
    "            #cat_remaining = rule_element[len(cat_direction):]  # rest of phrase after contains/not contains. \n",
    "            #   also remove any trailing 'and the', e.g. for wuc ends with in \"WUC begins with 14 AND ENDS WITH 00 OR 99 AND the CN contains \"BOOST\"\"\n",
    "            cat_remaining = re.sub(' THE$', '', match_type_split[jj].strip())\n",
    "            cat_remaining = re.sub(' AND$', '', cat_remaining).strip()\n",
    "            # remove trailing OR from contains/not contains cluse and take note\n",
    "            if re.search(' OR$', cat_remaining, flags=re.IGNORECASE):\n",
    "                cat_remaining = re.split(' OR$', cat_remaining, flags=re.IGNORECASE)[0].strip()\n",
    "                joined_by_and = False\n",
    "\n",
    "            if debug:\n",
    "                print 'cat_remaining: ' + cat_remaining\n",
    "            #cat_ANDs = cat_remaining.count(' AND ', re.IGNORECASE) # this is wrong if multiple cateogries linked by ANDs\n",
    "            cat_ORs = cat_remaining.count(' OR ')\n",
    "\n",
    "            # group all the ands into a list\n",
    "            cat_remaining_andsplit = re.split(' AND ', cat_remaining, flags=re.IGNORECASE)\n",
    "\n",
    "            # multiple category elements are linked by 'ands'.  remove these to avoid tricking the 'and' search later\n",
    "            # remove any empty strings, '', at the end of these categories\n",
    "            cat_remaining_andsplit = [element for element in cat_remaining_andsplit if element]\n",
    "\n",
    "            # if any ORs, split them off from the last item in the AND list\n",
    "            if cat_ORs > 0:\n",
    "                cat_remaining_orsplit = cat_remaining_andsplit.pop()\n",
    "                cat_remaining_orsplit = re.split(' OR ', cat_remaining_orsplit, flags=re.IGNORECASE)\n",
    "                # strip and remove quotes and parentheses from all the OR matches.\n",
    "                cat_pattern_ORs = [blurb.strip().strip(',').replace(\"\\\"\",\"\").replace(\"(\",\"\").replace(\")\",\"\") for blurb in cat_remaining_orsplit]\n",
    "                cat_pattern_ORs = '|'.join(cat_pattern_ORs)\n",
    "                # periods are wildcards, so escape\n",
    "                cat_pattern_ORs = cat_pattern_ORs.replace('.',r'\\.')\n",
    "                if debug:\n",
    "                    print 'filter for ' + cat_pattern_ORs\n",
    "\n",
    "                if 'CONTAINS' in match_type:\n",
    "                    # update indices: ensure new matches always match to the defined (e.g. WUC-starts-with) indice matches\n",
    "                    #  if joined by and: pending = intersection(pending, intersection(defined, new-matches))\n",
    "                    #  if joined by or: pending = union(pending, intersection(defined, new-matches))\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.contains(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE)) & (df.WUC_Rule.isnull())].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.contains(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE)) & (df.WUC_Rule.isnull())].index)))\n",
    "                elif 'NOT CONTAIN' in match_type:  # exclude if any words match\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[~(df[cat_field].str.contains(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE)) & (df.WUC_Rule.isnull())].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[~(df[cat_field].str.contains(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE)) & (df.WUC_Rule.isnull())].index)))\n",
    "                elif '=' in match_type or 'EQUALS' in match_type:\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.match(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE)) & (df[cat_field].str.len() == len(cat_pattern_ORs.split('|')[0])) & (df.WUC_Rule.isnull())].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.match(pat=cat_pattern_ORs,na=False,flags=re.IGNORECASE)) & (df[cat_field].str.len() == len(cat_pattern_ORs.split('|')[0])) & (df.WUC_Rule.isnull())].index)))                            \n",
    "                elif 'BEGINS WITH' in match_type:\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.startswith(tuple(cat_pattern_ORs.split('|')),na=False)) & (df.WUC_Rule.isnull())].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.startswith(tuple(cat_pattern_ORs.split('|')),na=False)) & (df.WUC_Rule.isnull())].index)))\n",
    "                else:  # ends with\n",
    "                    if debug:\n",
    "                        print 'else clause - OR match_type:' + match_type\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.endswith(tuple(cat_pattern_ORs.split('|')),na=False)) & (df.WUC_Rule.isnull())].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.endswith(tuple(cat_pattern_ORs.split('|')),na=False)) & (df.WUC_Rule.isnull())].index)))\n",
    "\n",
    "            if debug:\n",
    "                    print 'after matching any ORs: {} matches'.format(len(matching_indices_pending))\n",
    "\n",
    "            # strip and remove quotes from all the AND matches. escape periods (wildcards). keep a list\n",
    "            cat_pattern_ANDs = [blurb.strip().strip(',').replace(\"\\\"\",\"\").replace('.',r'\\.') for blurb in cat_remaining_andsplit]\n",
    "\n",
    "            # filter and phrases for this category\n",
    "            for cat_AND in cat_pattern_ANDs:\n",
    "                if debug:\n",
    "                    print 'filter for ' + cat_AND\n",
    "\n",
    "                if 'CONTAINS' in match_type:    \n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.contains(pat=cat_AND,na=False,flags=re.IGNORECASE)) & (df.WUC_Rule.isnull())].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.contains(pat=cat_AND,na=False,flags=re.IGNORECASE)) & (df.WUC_Rule.isnull())].index)))\n",
    "                elif 'NOT CONTAIN' in match_type:\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[~(df[cat_field].str.contains(pat=cat_AND,na=False,flags=re.IGNORECASE)) & (df.WUC_Rule.isnull())].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[~(df[cat_field].str.contains(pat=cat_AND,na=False,flags=re.IGNORECASE)) & (df.WUC_Rule.isnull())].index)))\n",
    "                elif '=' in match_type or 'EQUALS' in match_type:\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.match(pat=cat_AND,na=False,flags=re.IGNORECASE)) & (df.WUC_Rule.isnull())].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.match(pat=cat_AND,na=False,flags=re.IGNORECASE)) & (df.WUC_Rule.isnull())].index)))\n",
    "                elif 'BEGINS WITH' in match_type:\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.startswith(cat_AND,na=False)) & (df.WUC_Rule.isnull())].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.startswith(cat_AND,na=False)) & (df.WUC_Rule.isnull())].index)))\n",
    "                else:  # ends with\n",
    "                    if debug:\n",
    "                        print 'else clause - AND match_type: ' + match_type\n",
    "                    if joined_by_and:\n",
    "                        matching_indices_pending = matching_indices_pending.intersection(matching_indices_defined.intersection(set(df[(df[cat_field].str.endswith(cat_AND,na=False)) & (df.WUC_Rule.isnull())].index)))\n",
    "                    else:\n",
    "                        matching_indices_pending = matching_indices_pending.union(matching_indices_defined.intersection(set(df[(df[cat_field].str.endswith(cat_AND,na=False)) & (df.WUC_Rule.isnull())].index)))\n",
    "\n",
    "            if debug:\n",
    "                print 'after filtering match type: {} matches'.format(len(matching_indices_pending))\n",
    "\n",
    "        # finished looping through contains/not contains clauses for this field/category. update defined indices\n",
    "        matching_indices_defined = matching_indices_pending\n",
    "\n",
    "        if debug:\n",
    "            print 'after filtering category: {} matches'.format(len(matching_indices_defined))\n",
    "        if len(matching_indices_defined) == 0:\n",
    "            print 'rule {} changed 0 records to wuc {}: {}'.format(str(rule_name), new_wuc, wuc_narr)\n",
    "            return None\n",
    "\n",
    "    # filter\n",
    "    df = df.filter(items=matching_indices_defined, axis=\"index\")\n",
    "    if debug:\n",
    "        print 'matching_indices: ' + str(matching_indices_defined)\n",
    "        print 'filtered df: '\n",
    "        print df\n",
    "    # update wuc and rule\n",
    "    df.loc[:,'Work_Unit_Code'] = new_wuc\n",
    "    df.loc[:, 'WUC_Rule'] = df.WUC_Rule.apply(func=keep_rule_number, args=(rule_number,libraries))\n",
    "\n",
    "    print 'rule {} changed {:0,.0f} records to wuc {}: {}'.format(str(rule_name), df.shape[0], new_wuc, wuc_narr)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE TEST FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file names and rules\n",
    "rules_df = pd.read_csv('C://work//client-code//mdst//testing//file_rules.csv')\n",
    "rules_df.drop(rules_df.tail(1).index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create output tsvs\n",
    "def create_tsvs(df):\n",
    "    for index, row in df.iterrows():\n",
    "        rule = row['rule'][3:-3]\n",
    "        file_name = row['file_name']\n",
    "        \n",
    "        input_file = \"C://work//clockwork-etl//docker-compose//data/clockwork_etl//clockworkETL_workspace//mdst_c130//tests//resources//input//\" + file_name + \".csv\"\n",
    "        df_input = pd.read_csv(input_file, dtype={'Work_Unit_Code':'object'})\n",
    "        df = apply_rule(rule, df_input, pd.DataFrame(), libraries, FIELD_NAMES, FIELD_LOOKUP, debug=False)\n",
    "        df_input.update(df)\n",
    "        \n",
    "        output_file = \"C://work//clockwork-etl//docker-compose//data/clockwork_etl//clockworkETL_workspace//mdst_c130//tests//resources//output//\" + file_name + \".tsv\"\n",
    "        df_input.to_csv(output_file, sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule 738. AF41-3H-152 changed 2 records to wuc 41400: \n",
      "rule 613. AF41-3H-27 changed 4 records to wuc 41Y00: \n",
      "rule 613. AF41-3H-27 changed 3 records to wuc 41Y00: \n",
      "rule 1. AF11-3H-1 changed 3 records to wuc 11300: \n",
      "rule 2396. AF13-3HP-1 changed 4 records to wuc 13400: \n",
      "rule 2396. AF13-3HP-1 changed 2 records to wuc 13400: \n",
      "rule 2580. AF61-3HP-1 changed 6 records to wuc 61500: \n",
      "rule 2580. AF61-3HP-1 changed 7 records to wuc 61500: \n",
      "rule 2484. AF46-3HP-1 changed 2 records to wuc 46700: \n",
      "rule 3039. AF11-3DHP-150 changed 7 records to wuc 11299: \n",
      "rule 3039. AF11-3DHP-150 changed 4 records to wuc 11299: \n",
      "rule 3343. AF87-3DHP-1 changed 4 records to wuc 87T00: \n",
      "rule 3289. AF49-3DHP-17 changed 4 records to wuc 49100: \n",
      "rule 3289. AF49-3DHP-17 changed 3 records to wuc 49100: \n",
      "rule 3345. AF11-3L-1 changed 7 records to wuc 11400: \n",
      "rule 4032. AF78-3L-5 changed 2 records to wuc 78G00: \n",
      "rule 4032. AF78-3L-5 changed 2 records to wuc 78G00: \n",
      "rule 3966. AF71-3L-18 changed 4 records to wuc 71G00: \n",
      "rule 3966. AF71-3L-18 changed 3 records to wuc 71G00: \n",
      "rule 4142. AF11-3LP-1 changed 7 records to wuc 11100: \n",
      "rule 4268. AF71-3LP-6 changed 3 records to wuc 71300: \n",
      "rule 4323. AF76-3LP-14 changed 4 records to wuc 76B00: \n",
      "rule 4381. AF11-3DLP-1 changed 2 records to wuc 11400: \n",
      "rule 4459. AF22-3DLP-8 changed 0 records to wuc 22500: \n",
      "rule 4384. AF11-3DLP-4 changed 2 records to wuc 11200: \n",
      "rule 29769. SF11-34F4-1 changed 3 records to wuc 11190: \n",
      "rule 29769. SF11-34F4-1 changed 4 records to wuc 11190: \n",
      "rule 30185. SF42-34F4-9 changed 3 records to wuc 42DJ0: \n",
      "rule 30064. SF22-34F4-103 changed 2 records to wuc 22520: \n",
      "rule 30064. SF22-34F4-103 changed 4 records to wuc 22520: \n",
      "rule 31130. SF11-34F5-1 changed 3 records to wuc 11420: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\envs\\python27studiov1\\lib\\site-packages\\ipykernel_launcher.py:194: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule 31130. SF11-34F5-1 changed 2 records to wuc 11420: \n",
      "rule 32353. SF67-34F5-1 changed 2 records to wuc 67AB0: \n",
      "rule 32353. SF67-34F5-1 changed 2 records to wuc 67AB0: \n",
      "rule 31694. SF22-34F5-47 changed 3 records to wuc 22510: \n",
      "rule 31694. SF22-34F5-47 changed 4 records to wuc 22510: \n",
      "rule 32452. SF11-35F5-1 changed 4 records to wuc 114FH: \n",
      "rule 32452. SF11-35F5-1 changed 5 records to wuc 114FH: \n",
      "rule 33680. SF67-35F5-1 changed 3 records to wuc 67ABB: \n",
      "rule 33680. SF67-35F5-1 changed 2 records to wuc 67ABB: \n",
      "rule 32536. SF11-35F5-85 changed 3 records to wuc 11425: \n",
      "rule 32536. SF11-35F5-85 changed 3 records to wuc 11425: \n",
      "rule 4512. AF11-4H-1 changed 2 records to wuc 11120: \n",
      "rule 9992. AF86-4H-65 changed 2 records to wuc 86LL0: \n",
      "rule 6420. AF32-4H-41 changed 3 records to wuc 321D0: \n",
      "rule 6420. AF32-4H-41 changed 4 records to wuc 321D0: \n",
      "rule 10168. AF11-4HP-1 changed 3 records to wuc 11110: \n",
      "rule 10580. AF77-4HP-7 changed 2 records to wuc 77WA0: \n",
      "rule 10197. AF11-4HP-30 changed 3 records to wuc 11240: \n",
      "rule 10197. AF11-4HP-30 changed 2 records to wuc 11240: \n",
      "rule 10592. AF11-4L-1 changed 3 records to wuc 11190: \n",
      "rule 16286. AF71-4L-3 changed 1 records to wuc 713B0: \n",
      "rule 13259. AF41-4L-274 changed 3 records to wuc 41420: \n",
      "rule 18701. AF11-4LP-1 changed 5 records to wuc 11110: \n",
      "rule 18701. AF11-4LP-1 changed 5 records to wuc 11110: \n",
      "rule 19002. AF41-4LP-19 changed 2 records to wuc 41AC0: \n",
      "rule 18744. AF11-4LP-44 changed 4 records to wuc 11240: \n",
      "rule 18744. AF11-4LP-44 changed 6 records to wuc 11240: \n",
      "rule 33778. SF11-45F5-1 changed 1 records to wuc 1111P: \n",
      "rule 33778. SF11-45F5-1 changed 1 records to wuc 1111P: \n",
      "rule 833. SF76-45F5-1 changed 2 records to wuc 76AWG: \n",
      "rule 34011. SF11-45F5-234 changed 2 records to wuc 11524: \n",
      "rule 34011. SF11-45F5-234 changed 3 records to wuc 11524: \n",
      "rule 19182. AF11-5H-1 changed 2 records to wuc 1111P: \n",
      "rule 19182. AF11-5H-1 changed 2 records to wuc 1111P: \n",
      "rule 23082. AF67-5H-1 changed 2 records to wuc 67ABB: \n",
      "rule 23082. AF67-5H-1 changed 2 records to wuc 67ABB: \n",
      "rule 21887. AF42-5H-21 changed 2 records to wuc 4213E: \n",
      "rule 21887. AF42-5H-21 changed 2 records to wuc 4213E: \n",
      "rule 23424. AF11-5L-1 changed 2 records to wuc 1111J: \n",
      "rule 29410. AF71-5L-65 changed 2 records to wuc 71GFK: \n",
      "rule 29410. AF71-5L-65 changed 2 records to wuc 71GFK: \n",
      "rule 24863. AF13-5L-48 changed 2 records to wuc 1321R: \n",
      "rule 24863. AF13-5L-48 changed 3 records to wuc 1321R: \n"
     ]
    }
   ],
   "source": [
    "create_tsvs(rules_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
